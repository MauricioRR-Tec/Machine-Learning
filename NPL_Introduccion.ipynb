{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "\n",
    "<h2><center>Pre procesamiento de Textos</center></h2>\n",
    "\n",
    "---\n",
    "\n",
    "En el escenario actual, una forma de éxito de las personas se identifica por la forma en cómo se comunican y comparten información con los demás. \n",
    "\n",
    "Ahí es donde entran en escena los conceptos del lenguaje. Sin embargo, existen muchos idiomas en el mundo. Cada uno tiene muchos estándares y alfabetos, y la combinación de estas palabras organizadas de manera significativa resultó en la formación de una oración. Cada idioma tiene sus propias reglas al desarrollar estas oraciones y estos conjuntos de reglas también se conocen como gramática.\n",
    "\n",
    "----------\n",
    "\n",
    "En el mundo actual, según las estimaciones de la industria, solo el 20 por ciento de los datos se generan en el formato estructurado mientras hablamos, cuando twitteamos, cuando enviamos mensajes en WhatsApp, correo electrónico, Facebook, Instagram o cualquier mensaje de texto. Y la mayoría de estos datos existen en forma textual, que es un formato muy desestructurado. Para producir información significativa a partir de los datos de texto, debemos seguir un método llamado Análisis de texto.\n",
    "\n",
    "---------\n",
    "\n",
    "\n",
    "<h3><center>¿Qué es Text Mining?</center></h3>\n",
    "\n",
    "Text Mining es el proceso de derivar información significativa a partir de texto en lenguaje natural.\n",
    "\n",
    "---\n",
    "\n",
    "<h3><center>¿Qué es la NPL?</center></h3>\n",
    "\n",
    "El procesamiento del lenguaje natural (NPL) es un componente de la minería de textos que realiza un tipo especial de análisis lingüístico que esencialmente ayuda a una máquina a “leer” texto. \n",
    "\n",
    "Utiliza una metodología diferente para descifrar las ambigüedades en el lenguaje humano, que incluye lo siguiente: resumen automático, etiquetado de parte del discurso, desambiguación, fragmentación, así como desambiguación y comprensión y reconocimiento del lenguaje natural. Veremos todos los procesos paso a paso usando Python.\n",
    "\n",
    "----\n",
    "\n",
    "<h3><center>Terminologías en NPL</center></h3>\n",
    "\n",
    "### Tokenización\n",
    "\n",
    "La tokenización es el primer paso en la NPL. Es el proceso de romper cadenas en fichas que a su vez son pequeñas estructuras o unidades. La tokenización implica tres pasos que consisten en dividir una oración compleja en palabras, comprender la importancia de cada palabra con respecto a la oración y finalmente producir una descripción estructural en una oración de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Brazil',\n",
       " 'they',\n",
       " 'drive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right-hand',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " '.',\n",
       " 'Brazil',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'coastline',\n",
       " 'on',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'side',\n",
       " 'of',\n",
       " 'South',\n",
       " 'America']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus# sample text for performing tokenization\n",
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America\"# importing word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize# Passing the string text into word tokenize for breaking the sentences\n",
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 3, 'Brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1, ...})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('Brazil', 2),\n",
       " ('on', 2),\n",
       " ('side', 2),\n",
       " ('of', 2),\n",
       " ('In', 1),\n",
       " ('they', 1),\n",
       " ('drive', 1),\n",
       " ('right-hand', 1),\n",
       " ('road', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivado\n",
    "\n",
    "La derivación generalmente se refiere a normalizar las palabras en su forma básica o raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wait'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘giving’ \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(\"waiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited:wait\n",
      "waiting:wait\n",
      "waits:wait\n"
     ]
    }
   ],
   "source": [
    "# Checking for the list of words\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "    print(word+ \":\" +pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "given:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "for word in stm :\n",
    "    print(word+ \":\" +lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "En términos más simples, es el proceso de convertir una palabra a su forma básica. La diferencia entre la derivación y la lematización es que la lematización considera el contexto y convierte la palabra a su forma básica significativa, mientras que la derivación simplemente elimina los últimos caracteres, lo que a menudo conduce a significados incorrectos y errores ortográficos.\n",
    "\n",
    "Por ejemplo, la lematización identificaría correctamente la forma básica de \"caring\" a \"care\", mientras que la derivación cortaría la parte \"ing\" y la convertiría en automóvil (car)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from nltk) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\beyon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "# Importing Lemmatizer library from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stopwors from nltk library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words(‘english’))text = “Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.”\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(text1)stopwords = [x for x in text1 if x not in a]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "Las \"palabras vacías\" son las palabras más comunes en un idioma como \"el\", \"a\", \"en\", \"para\", \"arriba\", \"en\", \"es\", \"todos\". \n",
    "\n",
    "Estas palabras no proporcionan ningún significado y generalmente se eliminan de los textos. Podemos eliminar estas palabras vacías usando la biblioteca nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n",
      "['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n"
     ]
    }
   ],
   "source": [
    "# importing stopwors from nltk library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words('english'))\n",
    "\n",
    "text = \"Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.\"\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(text1)\n",
    "\n",
    "stopwords = [x for x in text1 if x not in a]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte del etiquetado de voz (POS)\n",
    "\n",
    "El etiquetado de la oración se utiliza para asignar cada palabra de un texto dado (como sustantivos, verbos, pronombres, adverbios, conjunción, adjetivos, interjección) según su definición y su contexto. \n",
    "\n",
    "Hay muchas herramientas disponibles para los etiquetadores POS y algunos de los etiquetadores más utilizados son NLTK, Spacy, TextBlob, Standford CoreNLP, etc.\n",
    "\n",
    "![img](https://miro.medium.com/max/700/1*JRej3tXz1qjoW_ZFSpfJNQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\beyon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vote', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('choose', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('particular', 'JJ')]\n",
      "[('man', 'NN')]\n",
      "[('or', 'CC')]\n",
      "[('a', 'DT')]\n",
      "[('group', 'NN')]\n",
      "[('(', '(')]\n",
      "[('party', 'NN')]\n",
      "[(')', ')')]\n",
      "[('to', 'TO')]\n",
      "[('represent', 'NN')]\n",
      "[('them', 'PRP')]\n",
      "[('in', 'IN')]\n",
      "[('parliament', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"vote to choose a particular man or a group (party) to represent them in parliament\"\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconocimiento de entidad nombrada\n",
    "\n",
    "Es el proceso de detectar las entidades nombradas, como el nombre de la persona, el nombre de la ubicación, el nombre de la empresa, las cantidades y el valor monetario.\n",
    "\n",
    "![img](https://miro.medium.com/max/527/1*jdt9-0S1lJm-caVKXNdyfQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\beyon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\beyon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m                             \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m                         )\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                                         \"https://docs.brew.sh/Installation then `brew install ghostscript`\")                \n\u001b[0;32m    818\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Google', 'NNP')]), ('’', 'NNP'), ('s', 'VBD'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event\"\n",
    "\n",
    "#importing chunk library from nltk\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# tokenize and POS Tagging before doing chunk\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragmentación\n",
    "\n",
    "(Chunking) significa recoger información individual y agruparla en partes más grandes. En el contexto de la NPL y la minería de texto, fragmentar significa agrupar palabras o tokens en fragmentos.\n",
    "\n",
    "![img](https://miro.medium.com/max/700/0*0sID7SOM5aDE6Lwq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"We saw the yellow dog\"\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "\n",
    "reg = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
